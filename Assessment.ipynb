{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a75136d5-5ed3-4608-bf81-f69c0440d685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Property Of Tausight - September 2023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Property Of Tausight - September 2023\n",
    "\"\"\"\n",
    "print(__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f6fe1c-75de-4925-bcd8-ba604c2ebcbb",
   "metadata": {},
   "source": [
    "# Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b72165-d1ba-4105-bbee-fdc2b7ad5487",
   "metadata": {},
   "source": [
    "#### This internship project aims to develop a robust Medical Entity Detection and Tracking model. \n",
    "\n",
    "In this interview assessment, we assume that you have already established a Named Entity Recognition (NER) model capable of extracting Personally Identifiable Information (PII) from medical documents, and its performance is considered acceptable. Your primary task is to enhance the capabilities of this model, enabling it to accurately associate the extracted PIIs with patients while distinguishing them from other PIIs (such as those related to doctors, nurses, etc.) and that whenever you detect and map PIIs to a patient, you add these mappings to a database. This database allows you to query patient information when needed. For this excersie, we recommend you to create your own database using a proper dat astructure and add your patients to it.\n",
    "\n",
    "\n",
    "\n",
    "### <font color='red'> To achieve this, you must tackle several critical challenges: </font>\n",
    "\n",
    "\n",
    "__Complex Patient Information:__ Patient PIIs extend beyond just a name and may include elements like Date of Birth (DOB), Medical Record Number (MRN), phone numbers, and addresses.\n",
    "\n",
    "__Repetitive PIIs:__ Patient PIIs can occur multiple times within a single document. For instance, a patient's name and MRN may appear in various sections of the document.\n",
    "\n",
    "__Dispersed Patient PIIs:__ Patient PIIs may not always be grouped together within the document; instead, they may be scattered throughout the text.\n",
    "\n",
    "__Recurrence of Healthcare Provider Names:__ The names of doctors and clinicians may also recur within the document, potentially complicating the accurate identification of patient PIIs.\n",
    "\n",
    "__Mutliple Patient PIIs:__ A document might contain multiple patients information at once.\n",
    "\n",
    "__Vague Patient Identity:__ A document might contains some of the patient PIIs and not all of them. \n",
    "    \n",
    "    Ex: You might find:\n",
    "    - Name = George Micheal, Phone = 61754584587\n",
    "    - Name = george Micheal, MRN = 4546546587\n",
    "    - Name = george Micheal, DOB = 01/01/1990\n",
    "    In this scenario, we are not sure if George Micheal is the same person within these two documetns. Its better to keep track of both.\n",
    "\n",
    "\n",
    "You are provided with a CSV file containing eight synthetic medical records, each with detected PIIs. Your primary objective is to develop a model that, given a set of detected PIIs, can precisely identify and associate the relevant information with the patient. It's crucial to note that your model will solely have access to the list of detected PIIs and not the actual text of the document. Consequently, you must devise a strategy to handle various scenarios where patient PIIs may be incomplete or distributed across different document sections.\n",
    "\n",
    "__Expected Model's Input:__ [OTHER, NAME, NAME, OTHER, ID, ......]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9986b93-6e4b-4811-a3fa-c7790b0e39d5",
   "metadata": {},
   "source": [
    "### Database Example:\n",
    "\n",
    "Your model detected:\n",
    "\n",
    "patient's name: Babak and DOB: 10/10/2021, but MRN is not detected. \n",
    "\n",
    "However, you have previously identified a patient with the name Babak, DOB: 10/10/2021, and MRN: 12457458 and added to the database.\n",
    "\n",
    "How can you leverage this prior knowledge to enhance your detection process?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ce0bb4-aad7-40a3-976e-f18f7dc84862",
   "metadata": {},
   "source": [
    "## Edge Cases to consider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afc2d8a-8fa0-422e-9ff7-1e7bcfbaf27e",
   "metadata": {},
   "source": [
    "Your model should be capable of addressing scenarios such as:\n",
    "\n",
    "* \"Wallace, Sarthak MRN: 546584 DOB: 01/01/1980 visited the hospital, .....\"\n",
    "* \"Sarthak DOB: 01/01/1980 visited the hospital, during his stay at the clinic he did receive his flu shot. MRN: 546584 and ....\"\n",
    "* \"Wallace, DOB: 01/01/1980 visited the hospital, .....\"\n",
    "\"Wallace, Sarthak visited the hospital, AM HOW OFTEN TO TAKE every day HOW TO TAKE INSTRUCTIONS mouth Start taMARAKI medication: MRN: 546584 On Discharge OMEPRAZOLE as: PRILOSEC 40 mg Last dose given 11/08/2010 08:03 AM two times a day mouth\"\n",
    "\n",
    "Your solution should comprehensively address these complexities, providing accurate patient PII mapping in diverse medical document scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b242b6aa-4eeb-4688-b948-8678f0a555b4",
   "metadata": {},
   "source": [
    "### Hint\n",
    "\n",
    "\n",
    "We would recommend you to create new examples using the provided data to ensure of functionality of your code. Example:\n",
    "\n",
    "    You code create new data from: text = \"Discharge Summary FINAL AHMADI, JOSEPH MRN: 22954568 DOB: 1956-01-08 60F Admission: 12/30/2015\"\n",
    "        - new text = \"Discharge Summary FINAL AHMADI, JOSEPH Phone: 61754854587 DOB: 1956-01-08 60F Admission: 12/30/2015\"\n",
    "        - new text = \"Discharge Summary FINAL AHMADI, JOSEPH Phone: 61754854587\"\n",
    "        \n",
    "    Be creative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdd597eb-a8b7-4e17-8041-c9a4200eaf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47011209-e54c-44de-8c35-bc8f8a4f262a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>PIIs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Discharge Summary FINAL AHMADI, JOSEPH MRN: 22...</td>\n",
       "      <td>['OTHER', 'OTHER', 'OTHER', 'NAME', 'NAME', 'O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DAVID KING M MRN: 15424576 DFCI MRN: 564756 DO...</td>\n",
       "      <td>['NAME', 'NAME', 'OTHER', 'OTHER', 'ID', 'OTHE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PATIENT MEDICATION LIST ON DISCHARGE FROM Mass...</td>\n",
       "      <td>['OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PATIENT MEDICATION LIST ON DISCHARGE FROM Mass...</td>\n",
       "      <td>['OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Discharge Summary FINAL AHMADI,  DOB: 1956-01-...</td>\n",
       "      <td>['OTHER', 'OTHER', 'OTHER', 'NAME', 'OTHER', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  Discharge Summary FINAL AHMADI, JOSEPH MRN: 22...   \n",
       "1  DAVID KING M MRN: 15424576 DFCI MRN: 564756 DO...   \n",
       "2  PATIENT MEDICATION LIST ON DISCHARGE FROM Mass...   \n",
       "3  PATIENT MEDICATION LIST ON DISCHARGE FROM Mass...   \n",
       "4  Discharge Summary FINAL AHMADI,  DOB: 1956-01-...   \n",
       "\n",
       "                                                PIIs  \n",
       "0  ['OTHER', 'OTHER', 'OTHER', 'NAME', 'NAME', 'O...  \n",
       "1  ['NAME', 'NAME', 'OTHER', 'OTHER', 'ID', 'OTHE...  \n",
       "2  ['OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', ...  \n",
       "3  ['OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', ...  \n",
       "4  ['OTHER', 'OTHER', 'OTHER', 'NAME', 'OTHER', '...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('sample_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14539138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b069c92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8921\n",
      "7057\n",
      "2310\n",
      "5057\n",
      "8821\n",
      "6989\n",
      "2310\n",
      "5057\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    p_row = data.iloc[i]\n",
    "    print(len(p_row['PIIs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaf96c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OTHER': 1, 'NAME': 2, 'ID': 3, 'DOB': 4, 'PHONE': 5, 'ADDRESS': 6, 'AGE': 7, 'DATE': 8}\n"
     ]
    }
   ],
   "source": [
    "# Here I build a dictionary in order to observe how many different PIIs there are\n",
    "PIIs_labels = {}\n",
    "replace_id = 1\n",
    "for i in range(8):\n",
    "    test_row = data.iloc[i]\n",
    "    test_PIIs = test_row['PIIs']\n",
    "    test_PIIs = test_PIIs.replace(\"[\", \"\").replace(\"]\", \"\").replace(\" \", \"\").replace(\"'\", \"\")\n",
    "    test_PIIs = test_PIIs.split(',')\n",
    "    for label in test_PIIs:\n",
    "        if label not in PIIs_labels:\n",
    "            PIIs_labels[label] = replace_id \n",
    "            replace_id  += 1\n",
    "print(PIIs_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d27cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing for PIIs\n",
    "# My idea here is to convert the tags in PIIs into strs in the same format as Text, separated by spaces.\n",
    "# Because I used fast tokenizers later, I couldn't input list[int] format or list[str], so I handled it like this.\n",
    "# Initial thought: Use the above dictionary to convert all PIIs into numerical expressions and continue in List format.\n",
    "# Due to the limitations, changes were made.\n",
    "new_data = data\n",
    "for i in range(8):\n",
    "    c_row = data.iloc[i]\n",
    "    c_PIIs = c_row['PIIs']\n",
    "    c_PIIs = c_PIIs.replace(\"[\", \"\").replace(\"]\", \"\").replace(\" \", \"\").replace(\"'\", \"\").replace(\",\", \" \")\n",
    "    new_data.iloc[i]['PIIs'] = c_PIIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c27a436b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  Discharge Summary FINAL AHMADI, JOSEPH MRN: 22...   \n",
      "1  DAVID KING M MRN: 15424576 DFCI MRN: 564756 DO...   \n",
      "2  PATIENT MEDICATION LIST ON DISCHARGE FROM Mass...   \n",
      "3  PATIENT MEDICATION LIST ON DISCHARGE FROM Mass...   \n",
      "4  Discharge Summary FINAL AHMADI,  DOB: 1956-01-...   \n",
      "5  DAVID M DFCI MRN: 564756 DOB: 11/12/1964 64F A...   \n",
      "6  PATIENT MEDICATION LIST ON DISCHARGE FROM Mass...   \n",
      "7  PATIENT MEDICATION LIST ON DISCHARGE FROM Mass...   \n",
      "\n",
      "                                                PIIs  \n",
      "0  OTHER OTHER OTHER NAME NAME OTHER ID OTHER DOB...  \n",
      "1  NAME NAME OTHER OTHER ID OTHER OTHER ID OTHER ...  \n",
      "2  OTHER OTHER OTHER OTHER OTHER OTHER ADDRESS OT...  \n",
      "3  OTHER OTHER OTHER OTHER OTHER OTHER OTHER OTHE...  \n",
      "4  OTHER OTHER OTHER NAME OTHER DOB OTHER OTHER O...  \n",
      "5  NAME OTHER OTHER OTHER ID OTHER DOB OTHER OTHE...  \n",
      "6  OTHER OTHER OTHER OTHER OTHER OTHER OTHER OTHE...  \n",
      "7  OTHER OTHER OTHER OTHER OTHER OTHER OTHER OTHE...  \n"
     ]
    }
   ],
   "source": [
    "print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21491d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2060, 2060, 2060, 2171, 2171, 2060, 8909, 2060, 2079, 2497, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2171, 2171, 2060, 8909, 2060, 2079, 2497, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 3042, 102]\n"
     ]
    }
   ],
   "source": [
    "# The main purpose here is to encode the content in Text and PIIs. \n",
    "# Because data needs to be provided during the model training process.\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "Text_input_ids = []\n",
    "Text_attention_masks = []\n",
    "PIIs_input_ids = []\n",
    "PIIs_attention_masks = []\n",
    "\n",
    "for i in range(8):\n",
    "    Text_tokens = new_data.iloc[i]['Text']\n",
    "    PIIs_tokens = new_data.iloc[i]['PIIs']\n",
    "    \n",
    "    Text_encode = tokenizer.encode_plus(Text_tokens, return_offsets_mapping=True, truncation=True, padding='max_length', max_length=64)\n",
    "    PIIs_encode = tokenizer.encode_plus(PIIs_tokens, return_offsets_mapping=True, truncation=True, padding='max_length', max_length=64)\n",
    "    # output of encode_plus: 'input_ids', 'attention_mask', 'offset_mapping'\n",
    "    \n",
    "#     if i == 0:\n",
    "#         print(PIIs_encode.input_ids)\n",
    "    \n",
    "    Text_input_ids.append(Text_encode['input_ids'])\n",
    "    Text_attention_masks.append(Text_encode['attention_mask'])\n",
    "    \n",
    "    PIIs_input_ids.append(PIIs_encode['input_ids'])\n",
    "    PIIs_attention_masks.append(PIIs_encode['attention_mask'])\n",
    "\n",
    "# Change data to tensor\n",
    "Text_input_ids = torch.tensor(Text_input_ids)\n",
    "Text_attention_masks = torch.tensor(Text_attention_masks)\n",
    "PIIs_input_ids = torch.tensor(PIIs_input_ids)\n",
    "PIIs_attention_masks = torch.tensor(PIIs_attention_masks)\n",
    "\n",
    "# print(Text_input_ids.shape)\n",
    "# print(Text_attention_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fec48b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset \n",
    "# Train：Validation：Test = 6:2\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "dataset_size = len(Text_input_ids)\n",
    "train_size = int(0.8 * dataset_size)  # Train dataset\n",
    "val_size = dataset_size - train_size  # Validation dataset\n",
    "\n",
    "full_dataset = TensorDataset(Text_input_ids, Text_attention_masks, PIIs_input_ids)\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52457a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at monologg/biobert_v1.1_pubmed and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Since the data is in the medical field, bioBert-NRE is used as a model for training. \n",
    "# This model can better handle models in the medical field.\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from torch.optim import Adam\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "model_name = \"monologg/biobert_v1.1_pubmed\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForTokenClassification.from_pretrained(model_name, num_labels=9999)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Optimzer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05e47afa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20, Training Loss: 9.1532, Validation Loss: 9.1276\n",
      "Epoch: 2/20, Training Loss: 9.1127, Validation Loss: 8.9824\n",
      "Epoch: 3/20, Training Loss: 8.9365, Validation Loss: 8.6563\n",
      "Epoch: 4/20, Training Loss: 8.5193, Validation Loss: 8.2707\n",
      "Epoch: 5/20, Training Loss: 8.0633, Validation Loss: 7.8114\n",
      "Epoch: 6/20, Training Loss: 7.6623, Validation Loss: 7.3430\n",
      "Epoch: 7/20, Training Loss: 7.2214, Validation Loss: 6.9362\n",
      "Epoch: 8/20, Training Loss: 6.8810, Validation Loss: 6.6320\n",
      "Epoch: 9/20, Training Loss: 6.5758, Validation Loss: 6.3868\n",
      "Epoch: 10/20, Training Loss: 6.3452, Validation Loss: 6.1793\n",
      "Epoch: 11/20, Training Loss: 6.2443, Validation Loss: 6.0043\n",
      "Epoch: 12/20, Training Loss: 5.9621, Validation Loss: 5.8571\n",
      "Epoch: 13/20, Training Loss: 5.8347, Validation Loss: 5.7359\n",
      "Epoch: 14/20, Training Loss: 5.7143, Validation Loss: 5.6429\n",
      "Epoch: 15/20, Training Loss: 5.5500, Validation Loss: 5.5669\n",
      "Epoch: 16/20, Training Loss: 5.3964, Validation Loss: 5.5040\n",
      "Epoch: 17/20, Training Loss: 5.3221, Validation Loss: 5.4506\n",
      "Epoch: 18/20, Training Loss: 5.2745, Validation Loss: 5.4109\n",
      "Epoch: 19/20, Training Loss: 5.2236, Validation Loss: 5.3829\n",
      "Epoch: 20/20, Training Loss: 5.1931, Validation Loss: 5.3677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./tokenizer\\\\tokenizer_config.json',\n",
       " './tokenizer\\\\special_tokens_map.json',\n",
       " './tokenizer\\\\vocab.txt',\n",
       " './tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataLoader\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_epochs = 20 # Num of Epoch\n",
    "\n",
    "# Learning Rate\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "warmup_steps = int(total_steps * 0.1)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# The begining of Training\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Process\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        inputs, masks, labels = batch\n",
    "        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    average_train_loss = total_train_loss / len(train_dataloader)\n",
    "    \n",
    "    # Validation Process\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            inputs, masks, labels = batch\n",
    "            inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs, attention_mask=masks, labels=labels)\n",
    "            total_val_loss += outputs.loss.item()\n",
    "\n",
    "    average_val_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Training Loss: {average_train_loss:.4f}, Validation Loss: {average_val_loss:.4f}\")\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model_path = \"./model.bin\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "tokenizer_path = \"./tokenizer\"\n",
    "tokenizer.save_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afa34dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3177, 7134, 551, 8361, 6039, 4479, 4075, 8234]\n",
      "De abilities ס cared vampires Jimmy heads interaction\n"
     ]
    }
   ],
   "source": [
    "# Text recognition\n",
    "\n",
    "# In fact, due to the need for decoding, the decoded text does not accurately correspond to the label.\n",
    "# I think it is a matter of time first. I may need more time to adjust the model parameters, \n",
    "# but my computer does not support me spending a lot of time calculating.\n",
    "\n",
    "# In addition, I have some ideas about the model.\n",
    "# For example, in order to prevent the label from becoming a list, I converted all the PIIs into text. \n",
    "# In fact, I think it is enough to convert the PIIs into the corresponding label through a dictionary,\n",
    "# and it is not even necessary with further coding, maybe this will be more accurate.\n",
    "\n",
    "model.eval()\n",
    "\n",
    "text = \"Discharge Summary FINAL AHMADI, JOSEPH Phone: 61754854587 DOB: 1956-01-08 60F Admission: 12/30/2015\"\n",
    "encoded_text = tokenizer.encode_plus(\n",
    "    text, \n",
    "    truncation=True, \n",
    "    padding='max_length', \n",
    "    max_length=8,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encoded_text[\"input_ids\"].to(device)\n",
    "attention_mask = encoded_text[\"attention_mask\"].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids, attention_mask=attention_mask)\n",
    "logits = output.logits\n",
    "\n",
    "predicted_label_ids = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "\n",
    "print(predicted_label_ids)\n",
    "\n",
    "predicted_labels = tokenizer.decode(predicted_label_ids, skip_special_tokens=True)\n",
    "\n",
    "print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed11b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
