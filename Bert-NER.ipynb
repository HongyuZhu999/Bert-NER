{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdd597eb-a8b7-4e17-8041-c9a4200eaf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47011209-e54c-44de-8c35-bc8f8a4f262a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>PIIs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Discharge Summary FINAL AHMADI, JOSEPH MRN: 22...</td>\n",
       "      <td>['OTHER', 'OTHER', 'OTHER', 'NAME', 'NAME', 'O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DAVID KING M MRN: 15424576 DFCI MRN: 564756 DO...</td>\n",
       "      <td>['NAME', 'NAME', 'OTHER', 'OTHER', 'ID', 'OTHE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PATIENT MEDICATION LIST ON DISCHARGE FROM Mass...</td>\n",
       "      <td>['OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PATIENT MEDICATION LIST ON DISCHARGE FROM Mass...</td>\n",
       "      <td>['OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Discharge Summary FINAL AHMADI,  DOB: 1956-01-...</td>\n",
       "      <td>['OTHER', 'OTHER', 'OTHER', 'NAME', 'OTHER', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  Discharge Summary FINAL AHMADI, JOSEPH MRN: 22...   \n",
       "1  DAVID KING M MRN: 15424576 DFCI MRN: 564756 DO...   \n",
       "2  PATIENT MEDICATION LIST ON DISCHARGE FROM Mass...   \n",
       "3  PATIENT MEDICATION LIST ON DISCHARGE FROM Mass...   \n",
       "4  Discharge Summary FINAL AHMADI,  DOB: 1956-01-...   \n",
       "\n",
       "                                                PIIs  \n",
       "0  ['OTHER', 'OTHER', 'OTHER', 'NAME', 'NAME', 'O...  \n",
       "1  ['NAME', 'NAME', 'OTHER', 'OTHER', 'ID', 'OTHE...  \n",
       "2  ['OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', ...  \n",
       "3  ['OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', ...  \n",
       "4  ['OTHER', 'OTHER', 'OTHER', 'NAME', 'OTHER', '...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('sample_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14539138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b069c92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8921\n",
      "7057\n",
      "2310\n",
      "5057\n",
      "8821\n",
      "6989\n",
      "2310\n",
      "5057\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    p_row = data.iloc[i]\n",
    "    print(len(p_row['PIIs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaf96c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OTHER': 1, 'NAME': 2, 'ID': 3, 'DOB': 4, 'PHONE': 5, 'ADDRESS': 6, 'AGE': 7, 'DATE': 8}\n"
     ]
    }
   ],
   "source": [
    "# Here I build a dictionary in order to observe how many different PIIs there are\n",
    "PIIs_labels = {}\n",
    "replace_id = 1\n",
    "for i in range(8):\n",
    "    test_row = data.iloc[i]\n",
    "    test_PIIs = test_row['PIIs']\n",
    "    test_PIIs = test_PIIs.replace(\"[\", \"\").replace(\"]\", \"\").replace(\" \", \"\").replace(\"'\", \"\")\n",
    "    test_PIIs = test_PIIs.split(',')\n",
    "    for label in test_PIIs:\n",
    "        if label not in PIIs_labels:\n",
    "            PIIs_labels[label] = replace_id \n",
    "            replace_id  += 1\n",
    "print(PIIs_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d27cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing for PIIs\n",
    "# My idea here is to convert the tags in PIIs into strs in the same format as Text, separated by spaces.\n",
    "# Because I used fast tokenizers later, I couldn't input list[int] format or list[str], so I handled it like this.\n",
    "# Initial thought: Use the above dictionary to convert all PIIs into numerical expressions and continue in List format.\n",
    "# Due to the limitations, changes were made.\n",
    "new_data = data\n",
    "for i in range(8):\n",
    "    c_row = data.iloc[i]\n",
    "    c_PIIs = c_row['PIIs']\n",
    "    c_PIIs = c_PIIs.replace(\"[\", \"\").replace(\"]\", \"\").replace(\" \", \"\").replace(\"'\", \"\").replace(\",\", \" \")\n",
    "    new_data.iloc[i]['PIIs'] = c_PIIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c27a436b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  Discharge Summary FINAL AHMADI, JOSEPH MRN: 22...   \n",
      "1  DAVID KING M MRN: 15424576 DFCI MRN: 564756 DO...   \n",
      "2  PATIENT MEDICATION LIST ON DISCHARGE FROM Mass...   \n",
      "3  PATIENT MEDICATION LIST ON DISCHARGE FROM Mass...   \n",
      "4  Discharge Summary FINAL AHMADI,  DOB: 1956-01-...   \n",
      "5  DAVID M DFCI MRN: 564756 DOB: 11/12/1964 64F A...   \n",
      "6  PATIENT MEDICATION LIST ON DISCHARGE FROM Mass...   \n",
      "7  PATIENT MEDICATION LIST ON DISCHARGE FROM Mass...   \n",
      "\n",
      "                                                PIIs  \n",
      "0  OTHER OTHER OTHER NAME NAME OTHER ID OTHER DOB...  \n",
      "1  NAME NAME OTHER OTHER ID OTHER OTHER ID OTHER ...  \n",
      "2  OTHER OTHER OTHER OTHER OTHER OTHER ADDRESS OT...  \n",
      "3  OTHER OTHER OTHER OTHER OTHER OTHER OTHER OTHE...  \n",
      "4  OTHER OTHER OTHER NAME OTHER DOB OTHER OTHER O...  \n",
      "5  NAME OTHER OTHER OTHER ID OTHER DOB OTHER OTHE...  \n",
      "6  OTHER OTHER OTHER OTHER OTHER OTHER OTHER OTHE...  \n",
      "7  OTHER OTHER OTHER OTHER OTHER OTHER OTHER OTHE...  \n"
     ]
    }
   ],
   "source": [
    "print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21491d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2060, 2060, 2060, 2171, 2171, 2060, 8909, 2060, 2079, 2497, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2171, 2171, 2060, 8909, 2060, 2079, 2497, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 3042, 102]\n"
     ]
    }
   ],
   "source": [
    "# The main purpose here is to encode the content in Text and PIIs. \n",
    "# Because data needs to be provided during the model training process.\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "Text_input_ids = []\n",
    "Text_attention_masks = []\n",
    "PIIs_input_ids = []\n",
    "PIIs_attention_masks = []\n",
    "\n",
    "for i in range(8):\n",
    "    Text_tokens = new_data.iloc[i]['Text']\n",
    "    PIIs_tokens = new_data.iloc[i]['PIIs']\n",
    "    \n",
    "    Text_encode = tokenizer.encode_plus(Text_tokens, return_offsets_mapping=True, truncation=True, padding='max_length', max_length=64)\n",
    "    PIIs_encode = tokenizer.encode_plus(PIIs_tokens, return_offsets_mapping=True, truncation=True, padding='max_length', max_length=64)\n",
    "    # output of encode_plus: 'input_ids', 'attention_mask', 'offset_mapping'\n",
    "    \n",
    "#     if i == 0:\n",
    "#         print(PIIs_encode.input_ids)\n",
    "    \n",
    "    Text_input_ids.append(Text_encode['input_ids'])\n",
    "    Text_attention_masks.append(Text_encode['attention_mask'])\n",
    "    \n",
    "    PIIs_input_ids.append(PIIs_encode['input_ids'])\n",
    "    PIIs_attention_masks.append(PIIs_encode['attention_mask'])\n",
    "\n",
    "# Change data to tensor\n",
    "Text_input_ids = torch.tensor(Text_input_ids)\n",
    "Text_attention_masks = torch.tensor(Text_attention_masks)\n",
    "PIIs_input_ids = torch.tensor(PIIs_input_ids)\n",
    "PIIs_attention_masks = torch.tensor(PIIs_attention_masks)\n",
    "\n",
    "# print(Text_input_ids.shape)\n",
    "# print(Text_attention_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fec48b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset \n",
    "# Train：Validation：Test = 6:2\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "dataset_size = len(Text_input_ids)\n",
    "train_size = int(0.8 * dataset_size)  # Train dataset\n",
    "val_size = dataset_size - train_size  # Validation dataset\n",
    "\n",
    "full_dataset = TensorDataset(Text_input_ids, Text_attention_masks, PIIs_input_ids)\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52457a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at monologg/biobert_v1.1_pubmed and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Since the data is in the medical field, bioBert-NRE is used as a model for training. \n",
    "# This model can better handle models in the medical field.\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from torch.optim import Adam\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "model_name = \"monologg/biobert_v1.1_pubmed\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForTokenClassification.from_pretrained(model_name, num_labels=9999)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Optimzer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05e47afa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20, Training Loss: 9.1532, Validation Loss: 9.1276\n",
      "Epoch: 2/20, Training Loss: 9.1127, Validation Loss: 8.9824\n",
      "Epoch: 3/20, Training Loss: 8.9365, Validation Loss: 8.6563\n",
      "Epoch: 4/20, Training Loss: 8.5193, Validation Loss: 8.2707\n",
      "Epoch: 5/20, Training Loss: 8.0633, Validation Loss: 7.8114\n",
      "Epoch: 6/20, Training Loss: 7.6623, Validation Loss: 7.3430\n",
      "Epoch: 7/20, Training Loss: 7.2214, Validation Loss: 6.9362\n",
      "Epoch: 8/20, Training Loss: 6.8810, Validation Loss: 6.6320\n",
      "Epoch: 9/20, Training Loss: 6.5758, Validation Loss: 6.3868\n",
      "Epoch: 10/20, Training Loss: 6.3452, Validation Loss: 6.1793\n",
      "Epoch: 11/20, Training Loss: 6.2443, Validation Loss: 6.0043\n",
      "Epoch: 12/20, Training Loss: 5.9621, Validation Loss: 5.8571\n",
      "Epoch: 13/20, Training Loss: 5.8347, Validation Loss: 5.7359\n",
      "Epoch: 14/20, Training Loss: 5.7143, Validation Loss: 5.6429\n",
      "Epoch: 15/20, Training Loss: 5.5500, Validation Loss: 5.5669\n",
      "Epoch: 16/20, Training Loss: 5.3964, Validation Loss: 5.5040\n",
      "Epoch: 17/20, Training Loss: 5.3221, Validation Loss: 5.4506\n",
      "Epoch: 18/20, Training Loss: 5.2745, Validation Loss: 5.4109\n",
      "Epoch: 19/20, Training Loss: 5.2236, Validation Loss: 5.3829\n",
      "Epoch: 20/20, Training Loss: 5.1931, Validation Loss: 5.3677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./tokenizer\\\\tokenizer_config.json',\n",
       " './tokenizer\\\\special_tokens_map.json',\n",
       " './tokenizer\\\\vocab.txt',\n",
       " './tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataLoader\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_epochs = 20 # Num of Epoch\n",
    "\n",
    "# Learning Rate\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "warmup_steps = int(total_steps * 0.1)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# The begining of Training\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Process\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        inputs, masks, labels = batch\n",
    "        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    average_train_loss = total_train_loss / len(train_dataloader)\n",
    "    \n",
    "    # Validation Process\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            inputs, masks, labels = batch\n",
    "            inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs, attention_mask=masks, labels=labels)\n",
    "            total_val_loss += outputs.loss.item()\n",
    "\n",
    "    average_val_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Training Loss: {average_train_loss:.4f}, Validation Loss: {average_val_loss:.4f}\")\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model_path = \"./model.bin\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "tokenizer_path = \"./tokenizer\"\n",
    "tokenizer.save_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afa34dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3177, 7134, 551, 8361, 6039, 4479, 4075, 8234]\n",
      "De abilities ס cared vampires Jimmy heads interaction\n"
     ]
    }
   ],
   "source": [
    "# Text recognition\n",
    "\n",
    "# In fact, due to the need for decoding, the decoded text does not accurately correspond to the label.\n",
    "# I think it is a matter of time first. I may need more time to adjust the model parameters, \n",
    "# but my computer does not support me spending a lot of time calculating.\n",
    "\n",
    "# In addition, I have some ideas about the model.\n",
    "# For example, in order to prevent the label from becoming a list, I converted all the PIIs into text. \n",
    "# In fact, I think it is enough to convert the PIIs into the corresponding label through a dictionary,\n",
    "# and it is not even necessary with further coding, maybe this will be more accurate.\n",
    "\n",
    "model.eval()\n",
    "\n",
    "text = \"Discharge Summary FINAL AHMADI, JOSEPH Phone: 61754854587 DOB: 1956-01-08 60F Admission: 12/30/2015\"\n",
    "encoded_text = tokenizer.encode_plus(\n",
    "    text, \n",
    "    truncation=True, \n",
    "    padding='max_length', \n",
    "    max_length=8,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encoded_text[\"input_ids\"].to(device)\n",
    "attention_mask = encoded_text[\"attention_mask\"].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids, attention_mask=attention_mask)\n",
    "logits = output.logits\n",
    "\n",
    "predicted_label_ids = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "\n",
    "print(predicted_label_ids)\n",
    "\n",
    "predicted_labels = tokenizer.decode(predicted_label_ids, skip_special_tokens=True)\n",
    "\n",
    "print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed11b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
